# Readme for run_all_Theo script 
Run gfp_gaussian for all files in dir (written for Theo's data sets)

## Usage 
```
run_all_theo.py [-h] -d DIR [-b PARAMETERS [PARAMETERS ...]]
                       [-suffix PREFIX [PREFIX ...]] [-o OUT] -c CSV_CONFIG
                       [-space SPACE] [-t TOL] [-noise NOISE] [--dryrun]
                       [--local] [--fallback] [--newparamfile] [-m] [-p]
```

## Options 
```
optional arguments:
  -h, --help            show this help message and exit
  -d DIR                directory with inpput files
  -b PARAMETERS [PARAMETERS ...]
                        Parameter file(s)
  -suffix PREFIX [PREFIX ...]
                        Parameter suffixs
  -o OUT                Output dir
  -c CSV_CONFIG         Csv config file
  -space SPACE          Space, log(default) or linear
  -t TOL                Tolerance of maximization
  -noise NOISE          Noise model
  --dryrun              Shows what will be done
  --local               Do not submit job, but run directly
  -m                    Run maximization
  -p                    Run prediction

```
The options `-c`, `-space`, `-t`, `-m`, `-p`, `-noise` behave as they do in direct `gfp_gaussian` runs. The `--dryrun` option shows what will be done without running anything. The other options are explained below.


### Input files
The option `-d` specifies the directory in which the input files are located. This will run all input files that end with `.csv`.

### Specifing parameter files
Either the `-b` or the `-suffix` flag needs to be set
###### Using `-b` flag
If the parameter file flag is ste to `infer`, each data set will be run using the generated parameter file that was generated by previous maximization runs. This is meant to be used for predictions. If eg parameter files `param1.txt`, `param2.txt`... are given those files will be used for all runs.

###### Using `-suffix` flag
If the suffix flag is set to `suffix1`,`suffix2`...  each input `example.csv` file should have a parameter file (or multiple) of the form: `example_suffi1.txt`, `example_suffi2.txt`...


## Running on the cluster (default) or locally
Unless the `--local` option is used, the script will (try to) submit jobs to slurm. For that it uses the template `submit_ggp_run.sl` which is in the same folder:
```
#!/bin/bash

#SBATCH --job-name=ggp
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=4G

#SBATCH --time=23:00:00
#SBATCH --qos=1day

#SBATCH --output=std.out
#SBATCH --mail-type=END,FAIL,TIME_LIMIT

${COMMAND}
```
All runs will submitted as individual jobs. Of course the slurm script can be modified if needed.

If the script is run locally, everything is run one after another directly.